{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTkZaZppnGFB"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeL-6jgKnGFE"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m94u-QO0nGFF"
      },
      "outputs": [],
      "source": [
        "from fastai.text import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t0_xU_TnGFH"
      },
      "source": [
        "## Transfer Raw AWD-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhYLc-zrnGFH",
        "outputId": "65b425f7-a198-4ab8-d7cf-870308adc746"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(#5) [Path('/home/sgugger/.fastai/data/imdb/unsup'),Path('/home/sgugger/.fastai/data/imdb/models'),Path('/home/sgugger/.fastai/data/imdb/train'),Path('/home/sgugger/.fastai/data/imdb/test'),Path('/home/sgugger/.fastai/data/imdb/README')]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "path = untar_data(URLs.IMDB)\n",
        "path.ls()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpcRjKK9nGFI",
        "outputId": "0d2f5fb9-0eaa-4056-8a49-2521bc953347"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(#4) [Path('/home/sgugger/.fastai/data/imdb/train/pos'),Path('/home/sgugger/.fastai/data/imdb/train/unsupBow.feat'),Path('/home/sgugger/.fastai/data/imdb/train/labeledBow.feat'),Path('/home/sgugger/.fastai/data/imdb/train/neg')]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(path/'train').ls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "im_31NR1nGFI"
      },
      "source": [
        "The data follows an ImageNet-style organization, in the train folder, we have two subfolders, `pos` and `neg` (for positive reviews and negative reviews). We can gather it by using the `TextDataLoaders.from_folder` method. The only thing we need to specify is the name of the validation folder, which is \"test\" (and not the default \"valid\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fbMQXhZnGFI"
      },
      "outputs": [],
      "source": [
        "dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlwdqH_qnGFI"
      },
      "source": [
        "We can then have a look at the data with the `show_batch` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsPJIf45nGFJ"
      },
      "outputs": [],
      "source": [
        "dls.show_batch(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QCk-2xonGFJ"
      },
      "source": [
        "We can see that the library automatically processed all the texts to split then in *tokens*, adding some special tokens like:\n",
        "\n",
        "- `xxbos` to indicate the beginning of a text\n",
        "- `xxmaj` to indicate the next word was capitalized\n",
        "\n",
        "Then, we can define a `Learner` suitable for text classification in one line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6uZHwBMnGFJ"
      },
      "outputs": [],
      "source": [
        "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrTwt0cDnGFK"
      },
      "source": [
        "We use the [AWD LSTM](https://arxiv.org/abs/1708.02182) architecture, `drop_mult` is a parameter that controls the magnitude of all dropouts in that model, and we use `accuracy` to track down how well we are doing. We can then fine-tune our pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LR Finder \n",
        "learn.lr_find()\n",
        "learn.recorder.plot(skip_end=15)"
      ],
      "metadata": {
        "id": "-kosFQh9nXUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwudUXDSnGFK",
        "outputId": "690597ca-f472-40f5-e3f2-1a5cdd748f52"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.587251</td>\n",
              "      <td>0.386230</td>\n",
              "      <td>0.828960</td>\n",
              "      <td>01:35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.307347</td>\n",
              "      <td>0.263843</td>\n",
              "      <td>0.892800</td>\n",
              "      <td>03:03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.215867</td>\n",
              "      <td>0.226208</td>\n",
              "      <td>0.911800</td>\n",
              "      <td>02:55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.155399</td>\n",
              "      <td>0.231144</td>\n",
              "      <td>0.913960</td>\n",
              "      <td>03:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.129277</td>\n",
              "      <td>0.200941</td>\n",
              "      <td>0.925920</td>\n",
              "      <td>03:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.fine_tune(5, 1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNsK5dqnGFK"
      },
      "source": [
        "Not too bad! To see how well our model is doing, we can use the `show_results` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yyxr3V2dnGFK"
      },
      "outputs": [],
      "source": [
        "learn.show_results(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ghsgP3snGFL"
      },
      "source": [
        "And we can predict on new texts quite easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plccadXrnGFL",
        "outputId": "1014b441-7e46-4a01-e66b-8ffee7c3b711"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('pos', tensor(1), tensor([0.0092, 0.9908]))"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "learn.predict(\"I really liked that movie!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmD4FGa6nGFL"
      },
      "source": [
        "Here we can see the model has considered the review to be positive. The second part of the result is the index of \"pos\" in our data vocabulary and the last part is the probabilities attributed to each class (99.1% for \"pos\" and 0.9% for \"neg\"). \n",
        "\n",
        "Now it's your turn! Write your own mini movie review, or copy one from the Internet, and we can see what this model thinks about it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vcbMYM8nGFL"
      },
      "source": [
        "### Data block API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLueTUCdnGFL"
      },
      "source": [
        "A datablock is built by giving the fastai library a bunch of information:\n",
        "\n",
        "- the types used, through an argument called `blocks`: here we have images and categories, so we pass `TextBlock` and `CategoryBlock`. To inform the library our texts are files in a folder, we use the `from_folder` class method.\n",
        "- how to get the raw items, here our function `get_text_files`.\n",
        "- how to label those items, here with the parent folder.\n",
        "- how to split those items, here with the grandparent folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEHdOMV4nGFM"
      },
      "outputs": [],
      "source": [
        "imdb = DataBlock(blocks=(TextBlock.from_folder(path), CategoryBlock),\n",
        "                 get_items=get_text_files,\n",
        "                 get_y=parent_label,\n",
        "                 splitter=GrandparentSplitter(valid_name='test'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBgtr7UPnGFM"
      },
      "source": [
        "This only gives a blueprint on how to assemble the data. To actually create it, we need to use the `dataloaders` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teMiKU3LnGFM"
      },
      "outputs": [],
      "source": [
        "dls = imdb.dataloaders(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvKhm15VnGFM"
      },
      "source": [
        "## ULMFiT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WhwTEOynGFM"
      },
      "source": [
        "The pretrained model we used in the previous section is called a language model. It was pretrained on Wikipedia on the task of guessing the next word, after reading all the words before. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could fine-tune our pretrained language model to the IMDb corpus and *then* use that as the base for our classifier.\n",
        "\n",
        "One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, in the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached in the unsup folder. We can use all of these reviews to fine tune the pretrained language model â€” this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles.\n",
        "\n",
        "The whole process is summarized by this picture:\n",
        "\n",
        "![ULMFit process](https://github.com/fastai/fastai/blob/master/nbs/images/ulmfit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CEXZnxDnGFN"
      },
      "source": [
        "### Fine-tuning a language model on IMDb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUGFubGVnGFN"
      },
      "source": [
        "We can get our texts in a `DataLoaders` suitable for language modeling very easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZc1L_45nGFO"
      },
      "outputs": [],
      "source": [
        "dls_lm = TextDataLoaders.from_folder(path, is_lm=True, valid_pct=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHy4tbwKnGFO"
      },
      "source": [
        "We need to pass something for `valid_pct` otherwise this method will try to split the data by using the grandparent folder names. By passing `valid_pct=0.1`, we tell it to get a random 10% of those reviews for the validation set.\n",
        "\n",
        "We can have a look at our data using `show_batch`. Here the task is to guess the next word, so we can see the targets have all shifted one word to the right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q89C7TRonGFO",
        "outputId": "09c8e7df-c36a-46ee-db1e-1e781e5df833"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>xxbos xxmaj about thirty minutes into the film , i thought this was one of the weakest \" xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . \\n\\n xxmaj but then there was a surprising twist that turned this episode into</td>\n",
              "      <td>xxmaj about thirty minutes into the film , i thought this was one of the weakest \" xxunk ever because it had the usual beginning ( a murder happening , then xxmaj columbo coming , inspecting everything and interrogating the main suspect ) squared ! xxmaj it was boring because i thought i knew everything already . \\n\\n xxmaj but then there was a surprising twist that turned this episode into a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>yeon . xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . \\n\\n i truly love this film . xxmaj if you have yet to see</td>\n",
              "      <td>. xxmaj these two girls were magical on the screen . i will certainly be looking into their other films . xxmaj xxunk xxmaj jeong - ah is xxunk cheerful and hauntingly evil as the stepmother . xxmaj finally , xxmaj xxunk - su xxmaj kim gives an excellent performance as the weary , broken father . \\n\\n i truly love this film . xxmaj if you have yet to see '</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tends to be tedious whenever there are n't any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj</td>\n",
              "      <td>to be tedious whenever there are n't any hideous monsters on display . xxmaj luckily the gutsy killings and eerie set designs ( by no less than xxmaj bill xxmaj paxton ! ) compensate for a lot ! a nine - headed expedition is send ( at hyper speed ) to the unexplored regions of space to find out what happened to a previously vanished spaceship and its crew . xxmaj bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>movie just sort of meanders around and nothing happens ( i do n't mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on</td>\n",
              "      <td>just sort of meanders around and nothing happens ( i do n't mean in terms of plot - no plot is fine , but no action ? xxmaj come on . ) xxmaj in hindsight , i should have expected this - after all , how much can really happen between 4 teens and a bear ? xxmaj so although special effects , acting , etc are more or less on par</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>greetings again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . \\n\\n xxmaj</td>\n",
              "      <td>again from the darkness . xxmaj writer / xxmaj director ( and xxmaj wes xxmaj anderson collaborator ) xxmaj noah xxmaj baumbach presents a semi - autobiographical therapy session where he unleashes the anguish and turmoil that has carried over from his childhood . xxmaj the result is an amazing insight into what many people go through in a desperate attempt to try and make their family work . \\n\\n xxmaj the</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dls_lm.show_batch(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6NB_FVknGFP"
      },
      "source": [
        "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture like before. We use accuracy and perplexity as metrics (the later is the exponential of the loss) and we set a default weight decay of 0.1. \n",
        "\n",
        "`to_fp16` puts the `Learner` in mixed precision, which is going to help speed up training on GPUs that have Tensor Cores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRw53pw2nGFP"
      },
      "outputs": [],
      "source": [
        "# Load model with pre-trained weight AWD_LSTM & mectrics acc + perplexity\n",
        "learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], path=path, wd=0.1).to_fp16()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je_MEO2onGFP"
      },
      "source": [
        "**By default, a pretrained `Learner` is in a frozen state, meaning that only the head of the model will train while the body stays frozen.** We show you what is behind the `fine_tune` method here and use a `fit_one_cycle` method to fit the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUDJZyatnGFP",
        "outputId": "30567b95-dfc8-474a-ff75-a2af62f95885"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>4.120048</td>\n",
              "      <td>3.912788</td>\n",
              "      <td>0.299565</td>\n",
              "      <td>50.038246</td>\n",
              "      <td>11:39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.fit_one_cycle(1, 1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCq44qQtnGFQ"
      },
      "source": [
        "This model takes a while to train, so it's a good opportunity to talk about saving intermediary results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvMdLuxLnGFQ"
      },
      "source": [
        "You can easily save the state of your model like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05QctRY_nGFQ"
      },
      "outputs": [],
      "source": [
        "learn.save('1epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73_GsSM0nGFR"
      },
      "source": [
        "It will create a file in `learn.path/models/` named \"1epoch.pth\". If you want to load your model on another machine after creating your `Learner` the same way, or resume training later, you can load the content of this file with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQR6rcRlnGFR"
      },
      "outputs": [],
      "source": [
        "# Load 1epoch\n",
        "learn = learn.load('1epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaPZyLL7nGFR",
        "outputId": "a1981bf6-a84e-49af-ac5f-af4db9b65ac3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>perplexity</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.893486</td>\n",
              "      <td>3.772820</td>\n",
              "      <td>0.317104</td>\n",
              "      <td>43.502548</td>\n",
              "      <td>12:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.820479</td>\n",
              "      <td>3.717197</td>\n",
              "      <td>0.323790</td>\n",
              "      <td>41.148880</td>\n",
              "      <td>12:30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.735622</td>\n",
              "      <td>3.659760</td>\n",
              "      <td>0.330321</td>\n",
              "      <td>38.851997</td>\n",
              "      <td>12:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.677086</td>\n",
              "      <td>3.624794</td>\n",
              "      <td>0.333960</td>\n",
              "      <td>37.516987</td>\n",
              "      <td>12:12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.636646</td>\n",
              "      <td>3.601300</td>\n",
              "      <td>0.337017</td>\n",
              "      <td>36.645859</td>\n",
              "      <td>12:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.553636</td>\n",
              "      <td>3.584241</td>\n",
              "      <td>0.339355</td>\n",
              "      <td>36.026001</td>\n",
              "      <td>12:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.507634</td>\n",
              "      <td>3.571892</td>\n",
              "      <td>0.341353</td>\n",
              "      <td>35.583862</td>\n",
              "      <td>12:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.444101</td>\n",
              "      <td>3.565988</td>\n",
              "      <td>0.342194</td>\n",
              "      <td>35.374371</td>\n",
              "      <td>12:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.398597</td>\n",
              "      <td>3.566283</td>\n",
              "      <td>0.342647</td>\n",
              "      <td>35.384815</td>\n",
              "      <td>12:11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.375563</td>\n",
              "      <td>3.568166</td>\n",
              "      <td>0.342528</td>\n",
              "      <td>35.451500</td>\n",
              "      <td>12:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Unfrezze and fine-tune it \n",
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(10, 1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izHmYqV-nGFR"
      },
      "source": [
        "Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the *encoder*. We can save it with `save_encoder`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EU0g34iBnGFR"
      },
      "outputs": [],
      "source": [
        "learn.save_encoder('finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg7CzTI0nGFR"
      },
      "source": [
        "> Jargon: Encoder: The model not including the task-specific final layer(s). It means much the same thing as *body* when applied to vision CNNs, but tends to be more used for NLP and generative models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XLh9xXInGFR"
      },
      "source": [
        "Before using this to fine-tune a classifier on the reviews, we can use our model to generate random reviews: since it's trained to guess what the next word of the sentence is, we can use it to write new reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3HsTxlFnGFS",
        "outputId": "d1a89b9d-a600-4a17-bc0a-b44a2914b50e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "TEXT = \"I liked this movie because\"\n",
        "N_WORDS = 40\n",
        "N_SENTENCES = 2\n",
        "preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n",
        "         for _ in range(N_SENTENCES)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5AVzS4gnGFS",
        "outputId": "1deee70b-d81c-4b86-f2c5-79f2684f7f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story\n",
            "i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the \" evil \" machine has to be used to protect\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\".join(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5ccE1IFnGFS"
      },
      "source": [
        "### Target task classifier fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOPBW0IBnGFS"
      },
      "source": [
        "We can gather our data for text classification almost exactly like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW-WbSlKnGFS"
      },
      "outputs": [],
      "source": [
        "dls_clas = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', text_vocab=dls_lm.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBihQL6nGFS"
      },
      "source": [
        "The main difference is that we have to use the exact same vocabulary as when we were fine-tuning our language model, or the weights learned won't make any sense. We pass that vocabulary with `text_vocab`.\n",
        "\n",
        "Then we can define our text classifier like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3C_qqGsxnGFS"
      },
      "outputs": [],
      "source": [
        "learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2trLWToynGFS"
      },
      "source": [
        "The difference is that before training it, we load the previous encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLBhM8NAnGFS"
      },
      "outputs": [],
      "source": [
        "learn = learn.load_encoder('finetuned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xka69DCnGFS"
      },
      "source": [
        "The last step is to train with discriminative learning rates and *gradual unfreezing*. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRRvq_JMnGFT",
        "outputId": "1fbf0c87-3caa-41f4-c651-dc93f31c38db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.347427</td>\n",
              "      <td>0.184480</td>\n",
              "      <td>0.929320</td>\n",
              "      <td>00:33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.fit_one_cycle(1, 2e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQY62qTPnGFT"
      },
      "source": [
        "In just one epoch we get the same result as our training in the first section, not too bad! We can pass `-2` to `freeze_to` to freeze all except the last two parameter groups:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbdWT4PjnGFT",
        "outputId": "d7ce1f59-9f69-4e20-cee9-452911a537f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.247763</td>\n",
              "      <td>0.171683</td>\n",
              "      <td>0.934640</td>\n",
              "      <td>00:37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.freeze_to(-2)\n",
        "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25k8VKvMnGFT"
      },
      "source": [
        "Then we can unfreeze a bit more, and continue training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ctEmrulnGFT",
        "outputId": "8c115b36-d42e-4578-db64-920beb321bf3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.193377</td>\n",
              "      <td>0.156696</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>00:45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.freeze_to(-3)\n",
        "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y5G_vSrnGFT"
      },
      "source": [
        "And finally, the whole model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwHAE3jUnGFT",
        "outputId": "70a84902-85f8-42c8-fa9b-343515ba920f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.172888</td>\n",
              "      <td>0.153770</td>\n",
              "      <td>0.943120</td>\n",
              "      <td>01:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.161492</td>\n",
              "      <td>0.155567</td>\n",
              "      <td>0.942640</td>\n",
              "      <td>00:57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "learn.unfreeze()\n",
        "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "final_umlfit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}